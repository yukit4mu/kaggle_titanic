# -*- coding: utf-8 -*-
"""titanic_YuaKitamura(21G1107007F)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R7Wo3zkBAuHaGtPUhLt6sWVmrU-hb32L

# データとライブラリの読み込み
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
# %matplotlib inline

#input data
train_data = pd.read_csv('/content/train.csv')
test_data = pd.read_csv('/content/test.csv')

train_data.info()

train_data.head()

"""# 各カラムの確認

## Survived
"""

train_data['Survived'].value_counts(normalize=True)

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='Survived', data=train_data)
plt.show()

"""## Pclass"""

train_data['Survived'].groupby(train_data['Pclass']).mean()

sns.countplot(x='Pclass', hue='Survived', data=train_data)

"""## Name"""

train_data['Name'].head()

train_data['Name_Status'] = train_data['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])
train_data['Name_Status'].value_counts()

"""## Name_Status（肩書き）"""

train_data['Survived'].groupby(train_data['Name_Status']).mean()

train_data['Name_Len'] = train_data['Name'].apply(lambda x: len(x))
train_data['Survived'].groupby(pd.qcut(train_data['Name_Len'],5)).mean()

pd.qcut(train_data['Name_Len'],5).value_counts()

"""## Sex"""

train_data['Sex'].value_counts(normalize=True)

train_data['Survived'].groupby(train_data['Sex']).mean()

"""## Age"""

train_data['Survived'].groupby(pd.qcut(train_data['Age'],5)).mean()

pd.qcut(train_data['Age'],5).value_counts()

"""## SibSp"""

train_data['Survived'].groupby(train_data['SibSp']).mean()

"""## Parch"""

train_data['Survived'].groupby(train_data['Parch']).mean()

"""## Ticket"""

train_data['Ticket_Len'] = train_data['Ticket'].apply(lambda x: len(x))
train_data['Ticket_Len'].value_counts()

train_data['Ticket_Lett'] = train_data['Ticket'].apply(lambda x: str(x)[0])
train_data['Ticket_Lett'].value_counts()

train_data.groupby(['Ticket_Lett'])['Survived'].mean()

"""## Fare"""

pd.qcut(train_data['Fare'], 3).value_counts()
train_data['Survived'].groupby(pd.qcut(train_data['Fare'], 3)).mean()

"""## Pclass"""

pd.crosstab(pd.qcut(train_data['Fare'], 5), columns=train_data['Pclass'])

"""## Cabin"""

train_data['Cabin_Letter'] = train_data['Cabin'].apply(lambda x: str(x)[0])
train_data['Cabin_Letter'].value_counts()
train_data['Survived'].groupby(train_data['Cabin_Letter']).mean()

train_data['Cabin_num'] = train_data['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])
train_data['Cabin_num'].replace('an', np.NaN, inplace = True)
train_data['Cabin_num'] = train_data['Cabin_num'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)
pd.qcut(train_data['Cabin_num'],3).value_counts()
train_data['Survived'].groupby(pd.qcut(train_data['Cabin_num'], 3)).mean()
train_data['Survived'].corr(train_data['Cabin_num'])

"""## Embarked"""

train_data['Embarked'].value_counts()
train_data['Embarked'].value_counts(normalize=True)
train_data['Survived'].groupby(train_data['Embarked']).mean()

sns.countplot(x='Embarked', hue='Pclass', data=train_data)

"""# 特徴量抽出に係るヘルパー関数の作成

## 名前の長さと敬称の特徴量抽出
"""

def names(train_data, test_data):
    for i in [train_data, test_data]:
        i['Name_Len'] = i['Name'].apply(lambda x: len(x))
        i['Name_Status'] = i['Name'].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])
        del i['Name']
    return train_data, test_data

"""## 年齢の欠損値の補完"""

def age_impute(train_data, test_data):
    # 共通処理：特徴量の選択とワンホットエンコーディング
    age_train_df = train_data[['Age', 'Pclass', 'Sex', 'Parch', 'SibSp']]
    age_test_df = test_data[['Age', 'Pclass', 'Sex', 'Parch', 'SibSp']]
    age_train_df = pd.get_dummies(age_train_df)
    age_test_df = pd.get_dummies(age_test_df)

    # トレーニングデータの既知の年齢でモデルをトレーニング
    known_age = age_train_df[age_train_df.Age.notnull()].values
    X = known_age[:, 1:]
    y = known_age[:, 0]
    rfr = RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1)
    rfr.fit(X, y)

    # トレーニングデータの未知の年齢を予測
    unknown_age_train = age_train_df[age_train_df.Age.isnull()].values
    predictedAges_train = rfr.predict(unknown_age_train[:, 1:])
    train_data.loc[train_data.Age.isnull(), 'Age'] = predictedAges_train

    # テストデータの未知の年齢を予測
    unknown_age_test = age_test_df[age_test_df.Age.isnull()].values
    predictedAges_test = rfr.predict(unknown_age_test[:, 1:])
    test_data.loc[test_data.Age.isnull(), 'Age'] = predictedAges_test

    return train_data, test_data

"""## 家族サイズ特徴量の計算"""

def fam_size(train_data, test_data):
    for i in [train_data, test_data]:
        i['Fam_Size'] = np.where((i['SibSp']+i['Parch']) == 0 , 'Solo',
                           np.where((i['SibSp']+i['Parch']) <= 3,'Nuclear', 'Big'))
        del i['SibSp']
        del i['Parch']
    return train_data, test_data

"""## チケット文字列の処理"""

def ticket_grouped(train_data, test_data):
    for i in [train_data, test_data]:
        i['Ticket_Lett'] = i['Ticket'].apply(lambda x: str(x)[0])
        i['Ticket_Lett'] = i['Ticket_Lett'].apply(lambda x: str(x))
        i['Ticket_Lett'] = np.where((i['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), i['Ticket_Lett'],
                                   np.where((i['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']),
                                            'Low_ticket', 'Other_ticket'))
        i['Ticket_Len'] = i['Ticket'].apply(lambda x: len(x))
        del i['Ticket']
    return train_data, test_data

"""## 客室情報の処理 -最初の文字"""

def cabin(train_data, test_data):
    for i in [train_data, test_data]:
        i['Cabin_Letter'] = i['Cabin'].apply(lambda x: str(x)[0])
        del i['Cabin']
    return train_data, test_data

"""## 客室情報の処理 -客室番号"""

def cabin_num(train_data, test_data):
    for i in [train_data, test_data]:
        i['Cabin_num1'] = i['Cabin'].apply(lambda x: str(x).split(' ')[-1][1:])
        i['Cabin_num1'].replace('an', np.NaN, inplace = True)
        i['Cabin_num1'] = i['Cabin_num1'].apply(lambda x: int(x) if not pd.isnull(x) and x != '' else np.NaN)
        i['Cabin_num'] = pd.qcut(train_data['Cabin_num1'],3)
    train_data = pd.concat((train_data, pd.get_dummies(train_data['Cabin_num'], prefix = 'Cabin_num')), axis = 1)
    test_data = pd.concat((test_data, pd.get_dummies(test_data['Cabin_num'], prefix = 'Cabin_num')), axis = 1)
    del train_data['Cabin_num']
    del test_data['Cabin_num']
    del train_data['Cabin_num1']
    del test_data['Cabin_num1']
    return train_data, test_data

"""## Embarked列の欠損値処理"""

def embarked_impute(train_data, test_data):
    for i in [train_data, test_data]:
        i['Embarked'] = i['Embarked'].fillna('S')
    return train_data, test_data

"""## Fare 列の欠損値の処理"""

test_data['Fare'].fillna(train_data['Fare'].mean(), inplace = True)

"""## ダミー変数への変換"""

def dummies(train_data, test_data, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett', 'Cabin_Letter', 'Name_Status', 'Fam_Size']):
    for column in columns:
        train_data[column] = train_data[column].apply(lambda x: str(x))
        test_data[column] = test_data[column].apply(lambda x: str(x))
        good_cols = [column+'_'+i for i in train_data[column].unique() if i in test_data[column].unique()]
        train_data = pd.concat((train_data, pd.get_dummies(train_data[column], prefix = column)[good_cols]), axis = 1)
        test_data = pd.concat((test_data, pd.get_dummies(test_data[column], prefix = column)[good_cols]), axis = 1)
        del train_data[column]
        del test_data[column]
    return train_data, test_data

"""## 不要なカラムの削除"""

def drop(train_data, test_data, bye = ['PassengerId']):
    for i in [train_data, test_data]:
        for z in bye:
            del i[z]
    return train_data, test_data

"""## モデルで使用するデータセットの整理"""

train_data = pd.read_csv('/content/train.csv')
test_data = pd.read_csv('/content/test.csv')
train_data, test_data = names(train_data, test_data)
train_data, test_data = age_impute(train_data, test_data)
train_data, test_data = cabin_num(train_data, test_data)
train_data, test_data = cabin(train_data, test_data)
train_data, test_data = embarked_impute(train_data, test_data)
train_data, test_data = fam_size(train_data, test_data)
test_data['Fare'].fillna(train_data['Fare'].mean(), inplace = True)
train_data, test_data = ticket_grouped(train_data, test_data)
train_data, test_data = dummies(train_data, test_data, columns = ['Pclass', 'Sex', 'Embarked', 'Ticket_Lett',
                                                                     'Cabin_Letter', 'Name_Status', 'Fam_Size'])
train_data, test_data = drop(train_data, test_data)

print(len(train_data.columns))

"""## ハイパーパラメータのチューニング + モデルの推定と評価"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(criterion='gini',
                             n_estimators=700,
                             min_samples_split=10,
                             min_samples_leaf=1,
                             max_features='auto',
                             oob_score=True,
                             random_state=1,
                             n_jobs=-1)
rf.fit(train_data.iloc[:, 1:], train_data.iloc[:, 0])
print("%.4f" % rf.oob_score_)

"""## 変数の重要度確認"""

pd.concat((pd.DataFrame(train_data.iloc[:, 1:].columns, columns = ['variable']),
           pd.DataFrame(rf.feature_importances_, columns = ['importance'])),
          axis = 1).sort_values(by='importance', ascending = False)[:20]

predictions = rf.predict(test_data)
predictions = pd.DataFrame(predictions, columns=['Survived'])
test_data = pd.read_csv('/content/test.csv')
predictions = pd.concat((test_data.iloc[:, 0], predictions), axis = 1)
predictions.to_csv('submisson.csv', sep=",", index = False)